{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Smote Method\n","\n","This notebook will apply the SMOTE balancing method to the training data."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Pre-sampling class imbalance:\n","0.0    0.5\n","1.0    0.5\n","Name: Diabetes_binary, dtype: float64\n"]}],"source":["import pandas as pd\n","\n","df_train = pd.read_csv('train.csv')\n","X_train = df_train.drop('Diabetes_binary', axis=1)\n","y_train = df_train['Diabetes_binary']\n","\n","val_df = pd.read_csv('val.csv')\n","X_val = val_df.drop('Diabetes_binary', axis=1)\n","y_val = val_df['Diabetes_binary']\n","\n","print(\"Pre-sampling class imbalance:\")\n","print(y_train.value_counts(normalize=True))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class distribution after SMOTE:\n","0.0    0.5\n","1.0    0.5\n","Name: Diabetes_binary, dtype: float64\n","New training set saved as 'train.csv'.\n"]}],"source":["import pandas as pd\n","from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(random_state=92)\n","X_smote, y_smote = smote.fit_resample(X_train, y_train)\n","\n","print(\"Class distribution after SMOTE:\")\n","print(pd.Series(y_smote).value_counts(normalize=True))\n","\n","columns = X_train.columns \n","df_smote = pd.DataFrame(X_smote, columns=columns)\n","df_smote['Diabetes_binary'] = y_smote  \n","\n","df_smote.to_csv('train.csv', index=False)\n","\n","print(\"New training set saved as 'train.csv'.\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Class distribution in the updated training set:\n","0.0    0.5\n","1.0    0.5\n","Name: Diabetes_binary, dtype: float64\n"]}],"source":["df = pd.read_csv('train.csv')\n","target_variable = 'Diabetes_binary'  # Change 'target' to your target column name if different\n","class_distribution = df[target_variable].value_counts(normalize=True)\n","X_train = df.drop('Diabetes_binary', axis=1)\n","y_train = df['Diabetes_binary']\n","\n","print(\"Class distribution in the updated training set:\")\n","print(class_distribution)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n","/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.95      0.67      0.79     43667\n","         1.0       0.28      0.80      0.42      7069\n","\n","    accuracy                           0.69     50736\n","   macro avg       0.62      0.73      0.60     50736\n","weighted avg       0.86      0.69      0.74     50736\n","\n"]}],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report\n","\n","# Initialize the MLPClassifier without class_weight (since it's not supported)\n","mlp = MLPClassifier(\n","    hidden_layer_sizes=(100, 50),  # More layers and neurons\n","    max_iter=1000,\n","    activation='relu',\n","    solver='adam',\n","    random_state=42,\n","    learning_rate_init=0.001,  # Default value, but you can tweak this\n","    early_stopping=True,  # To prevent overfitting\n","    validation_fraction=0.1,  # Fraction of training data to use as validation set for early stopping\n","    n_iter_no_change=10  # Number of iterations with no improvement to wait before stopping\n",")\n","\n","# Train the MLP on the resampled, scaled training data\n","mlp.fit(X_train, y_train)\n","\n","# Predict labels for the test set\n","y_pred = mlp.predict(X_val)\n","\n","# Evaluate the performance\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","\n","# Convert continuous values to binary class labels\n","#y_train_binary = (y_train > 0.5).astype(int)\n","\n","# Perform PCA with 2 components\n","#pca = PCA(n_components=4)\n","#X_train_pca = pca.fit_transform(X_train)\n","#X_val_pca = pca.transform(X_val)\n","\n","# Visualize the data\n","#plt.figure(figsize=(8, 6))\n","#plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train_binary, cmap='viridis')\n","#plt.xlabel('Principal Component 1')\n","#plt.ylabel('Principal Component 2')\n","#plt.title('PCA of Training Data')\n","#plt.colorbar(label='Class')\n","#plt.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[29965 13702]\n"," [ 1580  5489]]\n","Accuracy: 0.6987937559129612\n","Precision: 0.2860194883018081\n","Recall: 0.7764888951761211\n","F1 Score: 0.418050266565118\n"]}],"source":["# Convert continuous values to binary class labels\n","y_train_binary = (y_train > 0.5).astype(int)\n","\n","# Train SVM with linear kernel\n","svm_model = SVC(kernel='linear')\n","svm_model.fit(X_train, y_train_binary)\n","\n","# Test on validation set\n","y_pred = svm_model.predict(X_val)\n","\n","# Calculate evaluation metrics\n","conf_matrix = confusion_matrix(y_val, y_pred)\n","accuracy = accuracy_score(y_val, y_pred)\n","precision = precision_score(y_val, y_pred)\n","recall = recall_score(y_val, y_pred)\n","f1 = f1_score(y_val, y_pred)\n","\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1)"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Linear Kernel</h1>"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","class LinearSVM:\n","    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n","        self.lr = learning_rate\n","        self.lambda_param = lambda_param\n","        self.n_iters = n_iters\n","        self.w = None\n","        self.b = None\n","\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","        y_ = np.where(y <= 0, -1, 1)\n","        \n","        self.w = np.zeros(n_features)\n","        self.b = 0\n","\n","        for _ in range(self.n_iters):\n","            for idx, x_i in enumerate(X):\n","                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n","                if condition:\n","                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n","                else:\n","                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n","                    self.b -= self.lr * (-y_[idx])\n","\n","    def predict(self, X):\n","        linear_output = np.dot(X, self.w) - self.b\n","        return np.sign(linear_output)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [13], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Adjust these parameters as needed\u001b[39;00m\n\u001b[1;32m     29\u001b[0m svm \u001b[38;5;241m=\u001b[39m LinearSVM(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, lambda_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m y_pred_val \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation Data Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn [10], line 24\u001b[0m, in \u001b[0;36mLinearSVM.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_param \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambda_param\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_i, y_[idx]))\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39my_[idx])\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def print_classification_report(y_true, y_pred):\n","    \"\"\"\n","    Prints a simple classification report including precision, recall, and F1-score for each class.\n","    \"\"\"\n","    classes = np.unique(y_true)\n","    print(\"Class\\tPrecision\\tRecall\\t\\tF1-Score\")\n","    \n","    for cls in classes:\n","        tp = np.sum((y_pred == cls) & (y_true == cls))\n","        fp = np.sum((y_pred == cls) & (y_true != cls))\n","        fn = np.sum((y_pred != cls) & (y_true == cls))\n","        \n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","        \n","        print(f\"{cls}\\t{precision:.2f}\\t\\t{recall:.2f}\\t\\t{f1_score:.2f}\")\n","\n","def load_data(filename):\n","    data = np.genfromtxt(filename, delimiter=',', skip_header=1)\n","    X = data[:, 1:]  # Assuming the first column is the target\n","    y = data[:, 0]\n","    return X, y\n","\n","X_train, y_train = load_data('train.csv')\n","X_val, y_val = load_data('val.csv')\n","\n","# Adjust these parameters as needed\n","svm = LinearSVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n","svm.fit(X_train, y_train)\n","\n","y_pred_val = svm.predict(X_val)\n","\n","\n","print(\"\\nValidation Data Classification Report:\")\n","print_classification_report(y_val, y_pred_val)\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1>RDF kernel FAILLLL</h1>"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import numpy as np\n","import cvxopt\n","import cvxopt.solvers\n","import pandas as pd\n","\n","cvxopt.solvers.options['show_progress'] = False\n","\n","class SVM_RBF:\n","    def __init__(self, C=1.0, gamma=10):\n","        self.C = C\n","        self.gamma = float(gamma)\n","        self._support_vectors = None\n","        self._alphas = None\n","        self.intercept = None\n","        self._support_labels = None\n","\n","    def rbf_kernel(self, x, y):\n","        x = np.atleast_2d(x)\n","        y = np.atleast_2d(y)\n","        return np.exp(-self.gamma * np.sum((x[:, np.newaxis] - y) ** 2, axis=2))\n","\n","    def fit(self, data, labels):\n","        data_np = data.values if isinstance(data, pd.DataFrame) else data\n","        labels_np = labels.values if isinstance(labels, pd.Series) else labels\n","\n","        num_data, num_features = data_np.shape\n","        labels_np = labels_np.astype(np.double)\n","\n","        K = self.rbf_kernel(data_np, data_np)\n","\n","        P = cvxopt.matrix(np.outer(labels_np, labels_np) * K)\n","        q = cvxopt.matrix(np.ones(num_data) * -1)\n","        A = cvxopt.matrix(labels_np, (1, num_data), 'd')\n","        b = cvxopt.matrix(0.0)\n","\n","        G_max = cvxopt.matrix(np.diag(np.ones(num_data) * -1))\n","        G_min = cvxopt.matrix(np.diag(np.ones(num_data)))\n","        G = cvxopt.matrix(np.vstack((G_max, G_min)))\n","        h_max = cvxopt.matrix(np.zeros(num_data))\n","        h_min = cvxopt.matrix(np.ones(num_data) * self.C)\n","        h = cvxopt.matrix(np.vstack((h_max, h_min)))\n","\n","        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n","        alphas = np.ravel(solution['x'])\n","        sv_mask = alphas > 1e-5\n","        self._support_vectors = data_np[sv_mask]\n","        self._alphas = alphas[sv_mask]\n","        self._support_labels = labels_np[sv_mask]\n","\n","        # Calculate the intercept with corrected indexing\n","        sv_indices = np.where(sv_mask)[0]  # Indices of support vectors\n","        self.intercept = np.mean([\n","            y_k - np.sum(\n","                self._alphas * self._support_labels * K[i, sv_indices]\n","            ) for i, y_k in zip(sv_indices, self._support_labels)\n","        ])\n","\n","    def predict(self, X):\n","        if self._alphas is None or self._support_labels is None:\n","            raise ValueError(\"SVM model has not been trained. Call the 'fit' method first.\")\n","\n","        X_np = X.values if isinstance(X, pd.DataFrame) else X\n","        K = self.rbf_kernel(X_np, self._support_vectors)\n","        score = np.dot(K, self._alphas * self._support_labels) + self.intercept\n","        return np.sign(score)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM with RBF kernel - Classification Report on Validation Set (Subset):\n","Class\tPrecision\tRecall\t\tF1-Score\n","0.0\t0.86\t\t1.00\t\t0.92\n","1.0\t0.00\t\t0.00\t\t0.00\n"]}],"source":["def print_classification_report(y_true, y_pred):\n","    \"\"\"\n","    Prints a simple classification report including precision, recall, and F1-score for each class.\n","    \"\"\"\n","    classes = np.unique(y_true)\n","    print(\"Class\\tPrecision\\tRecall\\t\\tF1-Score\")\n","    \n","    for cls in classes:\n","        tp = np.sum((y_pred == cls) & (y_true == cls))\n","        fp = np.sum((y_pred == cls) & (y_true != cls))\n","        fn = np.sum((y_pred != cls) & (y_true == cls))\n","        \n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","        \n","        print(f\"{cls}\\t{precision:.2f}\\t\\t{recall:.2f}\\t\\t{f1_score:.2f}\")\n","\n","\n","\n","subset_size = 10000  # Adjust based on your dataset size and system capabilities\n","svm_rbf = SVM_RBF(C=1.0, gamma=0.1)\n","svm_rbf.fit(X_smote[:subset_size], y_smote[:subset_size])\n","y_pred_rbf = svm_rbf.predict(X_val[:subset_size])\n","print(\"SVM with RBF kernel - Classification Report on Validation Set (Subset):\")\n","print_classification_report(y_val[:subset_size], y_pred_rbf)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":2}

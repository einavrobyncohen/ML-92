{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "target = diabetes_binary['Diabetes_binary']\n",
    "features = diabetes_binary.drop(columns=['Diabetes_binary'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=92)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GITHUB\n",
    "https://github.com/KirillShmilovich/MLP-Neural-Network-From-Scratch/blob/master/MLP.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = np.squeeze(np.eye(10)[Y.astype(int).values.reshape(-1)])\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(10)[Y_val.astype(int).values.reshape(-1)])\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes =np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        # VCompute the sigmoid\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        # Compute softmax along the rows of the input\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        # Compute the loss along the rows, averaging along the number of samples\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        # Compute the accuracy along the rows, averaging along the number of samples\n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        # Compute the derivative of sigmoid where h=sigmoid(x)\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations \n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m target \u001b[38;5;241m=\u001b[39m diabetes_binary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiabetes_binary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m diabetes_binary\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiabetes_binary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mN_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[1;34m(self, X, Y, X_val, Y_val, L, N_l)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_sizes \u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m+\u001b[39m[N_l]\u001b[38;5;241m*\u001b[39mL\u001b[38;5;241m+\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]) \n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "Cell \u001b[1;32mIn[42], line 54\u001b[0m, in \u001b[0;36mMLP.__init_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_sizes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_sizes[i],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_sizes[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]]))\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "target = diabetes_binary['Diabetes_binary']\n",
    "features = diabetes_binary.drop(columns=['Diabetes_binary'])\n",
    "\n",
    "\n",
    "model = MLP(X_train,y_train,X_test,y_test,L=1,N_l=128)\n",
    "model.train(batch_size=8,epochs=25,lr=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium\n",
    "https://pub.towardsai.net/the-multilayer-perceptron-built-and-implemented-from-scratch-70d6b30f1964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n"
     ]
    }
   ],
   "source": [
    "#import required libaries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the MLP class used to feedforward and backpropagate the network across a defined number \n",
    "    of iterations and produce predictions. After iteration the predictions are assessed using \n",
    "    Binary Cross Entropy Cost function.  \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Running...')\n",
    "    \n",
    "    def __init__(self, design_matrix, Y, iterations=100000, lr=1e-1, input_layer = 2, hidden_layer = 3,output_layer =1):\n",
    "        self.design_matrix = design_matrix #design matrix attibute\n",
    "        self.iterations = iterations #iterations attibute\n",
    "        self.lr = lr #learning rate attibute\n",
    "        self.input_layer = input_layer #input layer attibute \n",
    "        self.hidden_layer = hidden_layer #hidden layer attibute\n",
    "        self.output_layer = output_layer #output layer attibute\n",
    "        self.weight_matrix_1 = np.random.randn(self.input_layer, self.hidden_layer) #weight attribute connecting to the hidden layer\n",
    "        self.weight_matrix_2 = np.random.randn(self.hidden_layer, self.output_layer)#weight attribute connecting to the output layer\n",
    "        self.cost = [] #cost list attribute \n",
    "        self.p_hats = [] #predictions list attribute\n",
    "        self.Y = Y\n",
    "\n",
    "    def sigmoid(self, x): # sigmoid function used at the hidden layer and output layer\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x): # sigmoid derivative used for backpropgation \n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def forward_propagation(self):#define function to feedforward the network \n",
    "        z = np.dot(self.design_matrix, self.weight_matrix_1) #linear transformation to the hidden layer\n",
    "        activation_func = self.sigmoid(z)#hidden layer activation function\n",
    "        zh = np.dot(activation_func, self.weight_matrix_2)#linear transformation to the output layer\n",
    "        p_hat = self.sigmoid(zh)#output layer prediction\n",
    "        return z, activation_func, zh, p_hat\n",
    "\n",
    "    def BCECost(self, y, p_hat): # binary cross entropy cost function\n",
    "        bce_cost = -(np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))) / len(y)\n",
    "        return bce_cost\n",
    "\n",
    "    def backword_prop(self, z_1, activation_func, z_2, p_hat): #backpropagation\n",
    "        del_2_1 = p_hat - self.Y\n",
    "        partial_deriv_2 = np.dot(activation_func.T, del_2_1) #∂loss/∂p *∂p/∂zh * ∂zh/∂wh\n",
    "        del_1_1 = del_2_1 \n",
    "        del_1_2 = np.multiply(del_1_1, self.weight_matrix_2.T) \n",
    "        del_1_3 = np.multiply(del_1_2, self.sigmoid_derivative(z_1))\n",
    "        partial_deriv_1 = np.dot(self.design_matrix.T, del_1_3) #∂loss/∂p * ∂p/∂zh * ∂zh/∂h * ∂h/∂z * ∂z/∂w\n",
    "        return partial_deriv_2, partial_deriv_1\n",
    "\n",
    "    def train(self):#train the network\n",
    "        for i in range(self.iterations): #loop based on number of iterations\n",
    "            z_1, activation_func, z_2, p_hat = self.forward_propagation()# feedforward\n",
    "            partial_deriv_2, partial_deriv_1 = self.backword_prop(z_1, activation_func, z_2, p_hat)#backpropgate\n",
    "            self.weight_matrix_1 = self.weight_matrix_1 - self.lr * partial_deriv_1#update weights connecting to the hidden layer (gradient descent)\n",
    "            self.weight_matrix_2 = self.weight_matrix_2 - self.lr * partial_deriv_2#update weights connecting to the output layer (gradient descent )\n",
    "            self.cost.append(self.BCECost(self.Y, p_hat))#store BCE cost in list\n",
    "            self.p_hats.append(p_hat)#store predictions in list\n",
    "        \n",
    "        \n",
    "        print('Training Complete')\n",
    "        print('----------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "sample = diabetes_binary.sample(200, random_state=92)\n",
    "target = sample['Diabetes_binary']\n",
    "features = sample.drop(columns=['Diabetes_binary'])\n",
    "x_trainsmall, x_testsmall, y_trainsmall, y_testsmall = train_test_split(features, target, test_size=0.2, random_state=92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (160, 160) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(x_trainsmall,y_trainsmall, input_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m#Pass data to the model (design matrix and y label)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 57\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations): \u001b[38;5;66;03m#loop based on number of iterations\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     z_1, activation_func, z_2, p_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation()\u001b[38;5;66;03m# feedforward\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     partial_deriv_2, partial_deriv_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackword_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#backpropgate\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_matrix_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_matrix_1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m partial_deriv_1\u001b[38;5;66;03m#update weights connecting to the hidden layer (gradient descent)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_matrix_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_matrix_2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m partial_deriv_2\u001b[38;5;66;03m#update weights connecting to the output layer (gradient descent )\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 46\u001b[0m, in \u001b[0;36mMLP.backword_prop\u001b[1;34m(self, z_1, activation_func, z_2, p_hat)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackword_prop\u001b[39m(\u001b[38;5;28mself\u001b[39m, z_1, activation_func, z_2, p_hat): \u001b[38;5;66;03m#backpropagation\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     del_2_1 \u001b[38;5;241m=\u001b[39m \u001b[43mp_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY\u001b[49m\n\u001b[0;32m     47\u001b[0m     partial_deriv_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(activation_func\u001b[38;5;241m.\u001b[39mT, del_2_1) \u001b[38;5;66;03m#∂loss/∂p *∂p/∂zh * ∂zh/∂wh\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     del_1_1 \u001b[38;5;241m=\u001b[39m del_2_1 \n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:2168\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2164\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   2167\u001b[0m ):\n\u001b[1;32m-> 2168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marraylike\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_ufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:276\u001b[0m, in \u001b[0;36marray_ufunc\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _standardize_out_kwarg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# for binary ops, use our custom dunder methods\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_dispatch_ufunc_to_dunder_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mops_dispatch.pyx:113\u001b[0m, in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:198\u001b[0m, in \u001b[0;36mOpsMixin.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__rsub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:6126\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6125\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:1384\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:6222\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[1;34m(self, result, name)\u001b[0m\n\u001b[0;32m   6219\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[0;32m   6220\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[0;32m   6221\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 6222\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6223\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   6225\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[0;32m   6226\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:656\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    653\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[0;32m    654\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 656\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    660\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\construction.py:715\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (160, 160) instead"
     ]
    }
   ],
   "source": [
    "mlp = MLP(x_trainsmall,y_trainsmall, input_layer=10) #Pass data to the model (design matrix and y label)\n",
    "mlp.train() #Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS DATA\n",
    "https://abtinmy.github.io/CS-SBU-NeuralNetwork/lectures/introduction/MLP-Scratch-Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights randomly\n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # initialize biases to 0\n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            # feedforward\n",
    "            layer1 = X.dot(self.weights1) + self.bias1\n",
    "            activation1 = sigmoid(layer1)\n",
    "            layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "            activation2 = sigmoid(layer2)\n",
    "            \n",
    "            # backpropagation\n",
    "            error = activation2 - y\n",
    "            d_weights2 = activation1.T.dot(error * sigmoid_derivative(layer2))\n",
    "            d_bias2 = np.sum(error * sigmoid_derivative(layer2), axis=0, keepdims=True)\n",
    "            error_hidden = error.dot(self.weights2.T) * sigmoid_derivative(layer1)\n",
    "            d_weights1 = X.T.dot(error_hidden)\n",
    "            d_bias1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "            \n",
    "            # update weights and biases\n",
    "            self.weights2 -= self.learning_rate * d_weights2\n",
    "            self.bias2 -= self.learning_rate * d_bias2\n",
    "            self.weights1 -= self.learning_rate * d_weights1\n",
    "            self.bias1 -= self.learning_rate * d_bias1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        layer1 = X.dot(self.weights1) + self.bias1\n",
    "        activation1 = sigmoid(layer1)\n",
    "        layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "        activation2 = sigmoid(layer2)\n",
    "        return (activation2 > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
    "y = y.reshape([150,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the MLP class\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# train the MLP on the training data\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "# evaluate the accuracy of the MLP\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "sample = diabetes_binary.sample(200, random_state=92)\n",
    "target = sample['Diabetes_binary']\n",
    "features = sample.drop(columns=['Diabetes_binary'])\n",
    "x_t, x_te, y_t, y_te = train_test_split(features, target, test_size=0.2, random_state=92)\n",
    "\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the 'keepdims' parameter is not supported in the pandas implementation of sum()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train the MLP on the training data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# make predictions on the test data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(x_te)\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, X, y, epochs)\u001b[0m\n\u001b[0;32m     32\u001b[0m error \u001b[38;5;241m=\u001b[39m activation2 \u001b[38;5;241m-\u001b[39m y\n\u001b[0;32m     33\u001b[0m d_weights2 \u001b[38;5;241m=\u001b[39m activation1\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(error \u001b[38;5;241m*\u001b[39m sigmoid_derivative(layer2))\n\u001b[1;32m---> 34\u001b[0m d_bias2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmoid_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m error_hidden \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m sigmoid_derivative(layer1)\n\u001b[0;32m     36\u001b[0m d_weights1 \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(error_hidden)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:11657\u001b[0m, in \u001b[0;36mDataFrame.sum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11648\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m  11649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[0;32m  11650\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11655\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11656\u001b[0m ):\n\u001b[1;32m> 11657\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:12503\u001b[0m, in \u001b[0;36mNDFrame.sum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  12495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[0;32m  12496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12497\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12501\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12502\u001b[0m ):\n\u001b[1;32m> 12503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_min_count_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12504\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnansum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m  12505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:12468\u001b[0m, in \u001b[0;36mNDFrame._min_count_stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  12456\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  12457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_min_count_stat_function\u001b[39m(\n\u001b[0;32m  12458\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12466\u001b[0m ):\n\u001b[0;32m  12467\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m], name\n\u001b[1;32m> 12468\u001b[0m     \u001b[43mnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  12470\u001b[0m     validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  12472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\numpy\\function.py:418\u001b[0m, in \u001b[0;36mvalidate_func\u001b[1;34m(fname, args, kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_stat_func(args, kwargs, fname\u001b[38;5;241m=\u001b[39mfname)\n\u001b[0;32m    417\u001b[0m validation_func \u001b[38;5;241m=\u001b[39m _validation_funcs[fname]\n\u001b[1;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidation_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\numpy\\function.py:88\u001b[0m, in \u001b[0;36mCompatValidator.__call__\u001b[1;34m(self, args, kwargs, fname, max_fname_arg_count, method)\u001b[0m\n\u001b[0;32m     86\u001b[0m     validate_kwargs(fname, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mvalidate_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fname_arg_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaults\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid validation method \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_validators.py:223\u001b[0m, in \u001b[0;36mvalidate_args_and_kwargs\u001b[1;34m(fname, args, kwargs, max_fname_arg_count, compat_args)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got multiple values for keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m         )\n\u001b[0;32m    222\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(args_dict)\n\u001b[1;32m--> 223\u001b[0m \u001b[43mvalidate_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_validators.py:165\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(fname, kwargs, compat_args)\u001b[0m\n\u001b[0;32m    163\u001b[0m kwds \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    164\u001b[0m _check_for_invalid_keys(fname, kwargs, compat_args)\n\u001b[1;32m--> 165\u001b[0m \u001b[43m_check_for_default_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_validators.py:81\u001b[0m, in \u001b[0;36m_check_for_default_values\u001b[1;34m(fname, arg_val_dict, compat_args)\u001b[0m\n\u001b[0;32m     78\u001b[0m     match \u001b[38;5;241m=\u001b[39m arg_val_dict[key] \u001b[38;5;129;01mis\u001b[39;00m compat_args[key]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter is not supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe pandas implementation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: the 'keepdims' parameter is not supported in the pandas implementation of sum()"
     ]
    }
   ],
   "source": [
    "# create an instance of the MLP class\n",
    "mlp = MLP(input_size=10, hidden_size=1, output_size=1)\n",
    "\n",
    "# train the MLP on the training data\n",
    "mlp.fit(x_t, y_t)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = mlp.predict(x_te)\n",
    "\n",
    "# evaluate the accuracy of the MLP\n",
    "print(f\"Accuracy: {accuracy_score(y_pred,y_te)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YT VID\n",
    "https://github.com/musikalkemist/DeepLearningForAudioWithPython/blob/master/8-%20Training%20a%20neural%20network%3A%20Implementing%20back%20propagation%20from%20scratch/code/mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2):\n",
    "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
    "            a variable number of hidden layers, and number of outputs\n",
    "\n",
    "        Args:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        # save derivatives per layer\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "        # save activations per layer\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # save the activations for backpropogation\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "            # save the activations for backpropogation\n",
    "            self.activations[i + 1] = activations\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_propagate(self, error):\n",
    "        \"\"\"Backpropogates an error signal.\n",
    "        Args:\n",
    "            error (ndarray): The error to backprop.\n",
    "        Returns:\n",
    "            error (ndarray): The final error of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate backwards through the network layers\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            # get activation for previous layer\n",
    "            activations = self.activations[i+1]\n",
    "\n",
    "            # apply sigmoid derivative function\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "\n",
    "            # reshape delta as to have it as a 2d array\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            current_activations = self.activations[i]\n",
    "\n",
    "            # reshape activations as to have them as a 2d column matrix\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            # save derivative after applying matrix multiplication\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "\n",
    "            # backpropogate the next error\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \"\"\"Trains model running forward prop and backprop\n",
    "        Args:\n",
    "            inputs (ndarray): X\n",
    "            targets (ndarray): Y\n",
    "            epochs (int): Num. epochs we want to train the network for\n",
    "            learning_rate (float): Step to apply to gradient descent\n",
    "        \"\"\"\n",
    "        # now enter the training loop\n",
    "        for i in range(epochs):\n",
    "            sum_errors = 0\n",
    "\n",
    "            # iterate through all the training data\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "\n",
    "                # activate the network!\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # keep track of the MSE for reporting later\n",
    "                sum_errors += self._mse(target, output)\n",
    "\n",
    "            # Epoch complete, report the training error\n",
    "            print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        print(\"=====\")\n",
    "\n",
    "\n",
    "    def gradient_descent(self, learningRate=1):\n",
    "        \"\"\"Learns by descending the gradient\n",
    "        Args:\n",
    "            learningRate (float): How fast to learn.\n",
    "        \"\"\"\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \"\"\"Sigmoid derivative function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        \"\"\"Mean Squared Error loss function\n",
    "        Args:\n",
    "            target (ndarray): The ground trut\n",
    "            output (ndarray): The predicted values\n",
    "        Returns:\n",
    "            (float): Output\n",
    "        \"\"\"\n",
    "        return np.average((target - output) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "sample = diabetes_binary.sample(200, random_state=92)\n",
    "target = sample['Diabetes_binary'].astype(int)\n",
    "features = sample.drop(columns=['Diabetes_binary'])\n",
    "x_t, x_te, y_t, y_te = train_test_split(features, target, test_size=0.2, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HighBP', 'HighChol', 'CholCheck', 'BMI', 'PhysActivity', 'Fruits',\n",
       "       'Veggies', 'AnyHealthcare', 'GenHlth', 'DiffWalk'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(\u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train network\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# get a prediction\u001b[39;00m\n\u001b[0;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mforward_propagate(y_te)\n",
      "Cell \u001b[1;32mIn[68], line 125\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, inputs, targets, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# iterate through all the training data\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[1;32m--> 125\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# activate the network!\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagate(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# create a Multilayer Perceptron with one hidden layer\n",
    "mlp = MLP(10, [3], 2)\n",
    "\n",
    "# train network\n",
    "mlp.train(features, target, 50, 0.1)\n",
    "\n",
    "# get a prediction\n",
    "output = mlp.forward_propagate(y_te)\n",
    "\n",
    "print()\n",
    "print(\"Our network believes that {} + {} is equal to {}\".format(input[0], input[1], output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_binary = pd.read_csv('features10csv.csv')\n",
    "sample = diabetes_binary.sample(200, random_state=92)\n",
    "target = sample['Diabetes_binary'].astype(int)\n",
    "features = sample.drop(columns=['Diabetes_binary'])\n",
    "x_t, x_te, y_t, y_te = train_test_split(features, target, test_size=0.2, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(x_t, y_t)\n",
    "y_pred = mlp.predict(x_te)\n",
    "accuracy_score(y_pred, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627554037185468"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some datascience website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, input_size, output_size, lr_rate):\n",
    "        self.x = None\n",
    "        self.w = np.random.randn(input_size, output_size) / np.sqrt(input_size)\n",
    "        rng = np.random.default_rng()\n",
    "        self.b = np.zeros(output_size)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lr_rate = lr_rate\n",
    "\n",
    "    def forward(self, x, is_train):\n",
    "        if is_train:\n",
    "            self.x = x\n",
    "        return np.dot(x, self.w) + self.b\n",
    "\n",
    "    def backward(self, dx):\n",
    "        dw = np.dot(self.x.T, dx)\n",
    "        db = np.sum(dx, axis=0)\n",
    "        self.w = self.w - np.dot(self.lr_rate, dw)\n",
    "        self.b = self.b - np.dot(self.lr_rate, db)\n",
    "        res = np.dot(dx, self.w.T)\n",
    "        return res\n",
    "\n",
    "    def save_parameters(self, params):\n",
    "        param_dict = {}\n",
    "        param_dict[\"layer\"] = \"Affine\"\n",
    "        param_dict[\"weights\"] = self.w.tolist()\n",
    "        param_dict[\"bias\"] = self.b.tolist()\n",
    "        param_dict[\"input_size\"] = self.input_size\n",
    "        param_dict[\"output_size\"] = self.output_size\n",
    "        param_dict[\"lr_rate\"] = self.lr_rate\n",
    "        params.append(param_dict)\n",
    "        return params\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x, is_train):\n",
    "        res = 1 / (1 + np.exp(-x))\n",
    "        if is_train:\n",
    "            self.out = res\n",
    "        return res\n",
    "\n",
    "    def backward(self, dx):\n",
    "        res = dx * (1 - self.out) * self.out\n",
    "        return res\n",
    "\n",
    "    def save_parameters(self, params):\n",
    "        param_dict = {}\n",
    "        param_dict[\"layer\"] = \"Sigmoid\"\n",
    "        params.append(param_dict)\n",
    "        return params\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.x = None\n",
    "        self.dx = np.zeros((input_size, output_size))\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x, is_train):\n",
    "        exp_sum = np.sum(np.exp(x), axis=1)\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i, :] = np.exp(x[i, :]) / exp_sum[i]\n",
    "\n",
    "        if is_train:\n",
    "            self.x = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, label):\n",
    "        for i in range(self.x.shape[0]):\n",
    "            if label[i] == 1:\n",
    "                self.dx[i] = np.array([self.x[i, 0] - 1, self.x[i, 1] - 0])\n",
    "            else:\n",
    "                self.dx[i] = np.array([self.x[i, 0] - 0, self.x[i, 1] - 1])\n",
    "\n",
    "        return self.dx\n",
    "\n",
    "    def save_parameters(self, params):\n",
    "        param_dict = {}\n",
    "        param_dict[\"layer\"] = \"Softmax\"\n",
    "        param_dict[\"input_size\"] = self.input_size\n",
    "        param_dict[\"output_size\"] = self.output_size\n",
    "        params.append(param_dict)\n",
    "        return params\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy:\n",
    "    def forward(self, x, label):\n",
    "        loss_sum = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            prob = x[i, :][0]\n",
    "            loss_sum += label[i] * np.log(prob) + (1 - label[i]) * np.log(1 - prob)\n",
    "\n",
    "        return (-1) * loss_sum / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    def __init__(self, layers, loss_layer, batch_size):\n",
    "        self.layers = layers\n",
    "        self.loss_layer = loss_layer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def predict(self, x, layers, is_train):\n",
    "        input_arr = x\n",
    "        for layer in layers:\n",
    "            res = layer.forward(input_arr, is_train)\n",
    "            input_arr = res\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_loss(self, x, label, layers, is_train):\n",
    "        res = self.predict(x, layers, is_train)\n",
    "        return self.loss_layer.forward(res, label)\n",
    "\n",
    "    def backward(self, label, layers):\n",
    "        dx = label\n",
    "        for layer in reversed(layers):\n",
    "            res = layer.backward(dx)\n",
    "            dx = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 2\n",
    "lr_rate = 0.001\n",
    "batch_size = 100\n",
    "input_layer = Affine(30, hidden_layer_size, lr_rate)\n",
    "sigmoid_layer_0 = Sigmoid()\n",
    "hidden_layer_0 = Affine(hidden_layer_size, hidden_layer_size, lr_rate)\n",
    "sigmoid_layer_1 = Sigmoid()\n",
    "hidden_layer_1 = Affine(hidden_layer_size, 2, lr_rate)\n",
    "softmax_layer = Softmax(batch_size, 2)\n",
    "loss_layer = BinaryCrossEntropy()\n",
    "layers = [\n",
    "        input_layer,\n",
    "        sigmoid_layer_0,\n",
    "        hidden_layer_0,\n",
    "        sigmoid_layer_1,\n",
    "        hidden_layer_1,\n",
    "        softmax_layer,\n",
    "    ]\n",
    "net = MultilayerPerceptron(layers, loss_layer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YT github\n",
    "https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/08%20Perceptron/perceptron.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def unit_step_func(x):\n",
    "    return np.where(x > 0 , 1, 0)\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_func = unit_step_func\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.where(y > 0 , 1, 0)\n",
    "\n",
    "        # learn weights\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activation_func(linear_output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.lr * (y_[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.activation_func(linear_output)\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added function to make randomforest work with sklearn\n",
    "def read_data_and_return_arrays(file_path, target_column):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract features\n",
    "    features = df.drop(columns=[target_column])\n",
    "    data = features.values\n",
    "    \n",
    "    # Extract target variable\n",
    "    target = df[target_column].astype(int).values\n",
    "    \n",
    "    # Extract target names if available\n",
    "    target_names = None\n",
    "    \n",
    "    return data, target, target_names\n",
    "\n",
    "data, target_rf, target_names = read_data_and_return_arrays(\n",
    "            file_path=\"features10csv.csv\", target_column='Diabetes_binary'\n",
    "        )\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron classification accuracy 0.6388213652191052\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(learning_rate=0.01, n_iters=100)\n",
    "p.fit(x_train, y_train)\n",
    "predictions = p.predict(x_test)\n",
    "\n",
    "print(\"Perceptron classification accuracy\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github\n",
    "https://github.com/rcalix1/DeepLearningAlgorithms/tree/main/SecondEdition/chapter4_Reg_MLP_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
